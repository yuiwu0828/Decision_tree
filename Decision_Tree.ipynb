{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    def __init__(self):\n",
    "        self.threshold = None\n",
    "        self.feature = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.predict = None\n",
    "        \n",
    "    def set_Node(self, f, t):\n",
    "        self.feature = f\n",
    "        self.threshold = t\n",
    "        \n",
    "    def adopt_child(self, n, choose):\n",
    "        if choose == 0:\n",
    "            self.left = n\n",
    "        elif choose == 1:\n",
    "            self.right = n\n",
    "            \n",
    "    def set_predict(self, p):\n",
    "        self.predict = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal(df):\n",
    "    features = df.columns\n",
    "    \n",
    "    for feature in features:\n",
    "        if feature == 'class':\n",
    "            continue\n",
    "        maximal = df[feature].max(axis=0)\n",
    "        minimal = df[feature].min(axis=0)\n",
    "        for i in range(len(df[feature])):\n",
    "            if not df[feature][i]:\n",
    "                continue\n",
    "            else:\n",
    "                df[feature][i] = (df[feature][i] - minimal) / (maximal - minimal)\n",
    "    return df\n",
    "\n",
    "def row_distance(a, b, df):\n",
    "    distance = 0\n",
    "    features = df.columns\n",
    "    for feature in features:\n",
    "        if feature == 'class':\n",
    "            continue\n",
    "        elif df[feature][b] == -1 or df[feature][a] == -1:\n",
    "            distance = 100000\n",
    "        else:\n",
    "            distance += (df[feature][a] - df[feature][b])**2  \n",
    "    return math.sqrt(distance)\n",
    "\n",
    "def Get_Neighbors(df, a):\n",
    "    distance = []\n",
    "    data = []\n",
    "    for i in range(len(df.index)):\n",
    "        dist = row_distance(i, a, df)\n",
    "        distance.append(dist)\n",
    "        data.append(i)\n",
    "    distance = np.array(distance)\n",
    "    data = np.array(data)\n",
    "    index_dist = distance.argsort()\n",
    "    data = data[index_dist]\n",
    "    l = math.floor(len(df.index) * 0.05);\n",
    "    neighbors = data[:l]\n",
    "    return neighbors\n",
    "                            \n",
    "\n",
    "def distance(df):\n",
    "    features = df.columns\n",
    "    df = df.replace(np.nan, -1)\n",
    "    for feature in features:\n",
    "        for i in range(len(df.index)):\n",
    "            if df[feature][i] == -1:\n",
    "                n = Get_Neighbors(df, i)\n",
    "                df[feature][i] = 0.0\n",
    "                #print(len(n))\n",
    "                for j in n:\n",
    "                    df[feature][i] += 1 / len(n) * df[feature][j]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_value_mean(df):\n",
    "    features = df.columns\n",
    "    \n",
    "    for feature in features:\n",
    "        mean = df_x[feature].mean()\n",
    "        df[feature] = df[feature].fillna(mean)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def remove_missing_value(df):\n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Entropy(df):\n",
    "    result = df[\"class\"].value_counts(normalize=True)\n",
    "    e = 0.0\n",
    "    \n",
    "    for i in result:\n",
    "        e -= i * math.log(i, 2)\n",
    "        \n",
    "    return e\n",
    "\n",
    "def IG(df, df1, df2):\n",
    "    p1 = df1[\"class\"].count()\n",
    "    p = df[\"class\"].count()\n",
    "    gain = Entropy(df) - ((p1 / p) * Entropy(df1)) - ((1 - p1 / p) * Entropy(df2))    \n",
    "    \n",
    "    return gain\n",
    "\n",
    "def max_feature_gain(df, feature):\n",
    "    maximal = -1\n",
    "    threshold = 0\n",
    "    \n",
    "    for i in set(df[feature]):\n",
    "        gain = IG(df, df[df[feature] <= i], df[df[feature] > i])\n",
    "        if gain > maximal:\n",
    "            maximal = gain\n",
    "            threshold = i\n",
    "            \n",
    "    return maximal, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_data(df, seed):\n",
    "    return df.sample(frac = 0.2, random_state = seed)\n",
    "\n",
    "def random_feature(features):\n",
    "    length = math.floor(len(features) * 0.5)\n",
    "    return np.random.choice(features, length, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_tree(data, features):\n",
    "    feature = \"\"\n",
    "    threshold = 0\n",
    "    maximal = -1\n",
    "    \n",
    "    for i in features:\n",
    "        val, standard = max_feature_gain(data, i)\n",
    "        if val > maximal:\n",
    "            maximal = val\n",
    "            threshold = standard\n",
    "            feature = i\n",
    "    \n",
    "    node = Tree()\n",
    "    node.set_Node(feature, threshold)\n",
    "    if maximal > 0:\n",
    "        node.adopt_child(construct_tree(data[data[feature] <= threshold], features), 0)\n",
    "        node.adopt_child(construct_tree(data[data[feature] >  threshold], features), 1)\n",
    "        \n",
    "    if node.left is None and node.right is None:\n",
    "        predicts = data[\"class\"].value_counts()\n",
    "        label_num = 0\n",
    "        label = 0\n",
    "        for k, j in predicts.items():\n",
    "            if j > label_num:\n",
    "                label_num = j\n",
    "                label = k\n",
    "        node.set_predict(label)\n",
    "        \n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(predictions, y_test):\n",
    "    print('Accuracy: %s' % accuracy_score(y_test, predictions))\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply(row, node):\n",
    "    if node.predict is not None:\n",
    "        return node.predict\n",
    "    elif row[node.feature] <= node.threshold:\n",
    "        return apply(row, node.left)\n",
    "    else:\n",
    "        return apply(row, node.right)\n",
    "\n",
    "def fit(df, forest):\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    predict_list = []\n",
    "    for i, row in df.iterrows():\n",
    "        count += 1\n",
    "        ans = []\n",
    "        for j in forest:\n",
    "            re = apply(row, j)\n",
    "            ans.append(re)\n",
    "        vote = max(set(ans), key = ans.count)\n",
    "        predict_list.append(vote)\n",
    "    report(predict_list, df['class'].tolist())\n",
    "        #if vote == df[\"class\"][i]:\n",
    "            #correct += 1\n",
    "    #print('fit: ', correct / count)\n",
    "    return correct / count\n",
    "\n",
    "def cv(df, root):\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for i, row in df.iterrows():\n",
    "        count += 1\n",
    "        ans = []\n",
    "        re = apply(row, root)\n",
    "        if re == df[\"class\"][i]:\n",
    "            correct += 1\n",
    "    #print('cv: ', correct / count)\n",
    "    return correct / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decision_Tree(train, test, features):\n",
    "    #train = data.sample(frac = 0.7, random_state = 1)\n",
    "    #test = data.drop(train.index)\n",
    "        \n",
    "    features_valid = []\n",
    "    for j in features:\n",
    "        if j == 'class':\n",
    "            continue\n",
    "        features_valid.append(j)\n",
    "    forest = []\n",
    "    \n",
    "    count = 0\n",
    "    while len(forest) < 10:\n",
    "        df = random_data(train, count)\n",
    "        feature_set = random_feature(features_valid)\n",
    "        root = construct_tree(df, feature_set)\n",
    "        if cv(test, root) > 0.45:\n",
    "            forest.append(root)\n",
    "        count += 1\n",
    "        \n",
    "    acc_test = fit(test, forest)\n",
    "    return forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
      "0          0.218182          0.090909         0.40        0.028169   0.085284   \n",
      "1          0.290909          0.396694         0.26        0.056338   0.113712   \n",
      "2          0.463636          0.471074         0.32        0.091549   0.127090   \n",
      "3          0.672727          0.214876         0.56        0.063380   0.135452   \n",
      "4          0.163636          0.429752         0.21        0.042254   0.115385   \n",
      "...             ...               ...          ...             ...        ...   \n",
      "1018       0.218182          0.347107         0.09        0.056338   0.115385   \n",
      "1019       0.509091          0.350754         0.48        0.093296   0.066890   \n",
      "1020       0.290909          0.338843         0.04        0.035211   0.107023   \n",
      "1021       0.245455          0.256198         0.24        0.091549   0.110368   \n",
      "1022       0.409091          0.314050         0.30        0.049296   0.088629   \n",
      "\n",
      "      free_sulfur_dioxide  total_sulfur_dioxide   density        pH  \\\n",
      "0                0.233635              0.215548  0.370645  0.598425   \n",
      "1                0.461538              0.441696  0.446256  0.322371   \n",
      "2                0.323077              0.300353  0.570793  0.433071   \n",
      "3                0.076923              0.063604  0.670867  0.314961   \n",
      "4                0.200000              0.088339  0.495923  0.669291   \n",
      "...                   ...                   ...       ...       ...   \n",
      "1018             0.277484              0.035336  0.340252  0.543307   \n",
      "1019             0.061538              0.010601  0.324685  0.362205   \n",
      "1020             0.246154              0.088339  0.459600  0.464567   \n",
      "1021             0.400000              0.215548  0.465530  0.677165   \n",
      "1022             0.107692              0.038869  0.558933  0.456693   \n",
      "\n",
      "      sulphates   alcohol  class  \n",
      "0      0.159509  0.482143      5  \n",
      "1      0.092025  0.267857      5  \n",
      "2      0.153374  0.303571      5  \n",
      "3      0.208589  0.446429      6  \n",
      "4      0.177914  0.308824      5  \n",
      "...         ...       ...    ...  \n",
      "1018   0.134969  0.553571      6  \n",
      "1019   0.196319  0.660714      7  \n",
      "1020   0.167640  0.285714      6  \n",
      "1021   0.134969  0.482143      6  \n",
      "1022   0.208589  0.375000      6  \n",
      "\n",
      "[1023 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "df_x = pd.read_excel(\"Dataset1/train/X_train.xlsx\")\n",
    "df_y = pd.read_excel(\"Dataset1/train/y_train.xlsx\")\n",
    "#df_x = fill_missing_value_mean(df_x)\n",
    "df_x = normal(df_x)\n",
    "df_x = distance(df_x)\n",
    "data = df_x.join(df_y)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5588235294117647\n",
      "Confusion Matrix:\n",
      "[[ 0  2  3  0  0]\n",
      " [ 0 25  7  0  0]\n",
      " [ 0 16 32  1  0]\n",
      " [ 0  3 11  0  0]\n",
      " [ 0  1  0  1  0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.00      0.00      0.00         5\n",
      "           5       0.53      0.78      0.63        32\n",
      "           6       0.60      0.65      0.63        49\n",
      "           7       0.00      0.00      0.00        14\n",
      "           8       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.56       102\n",
      "   macro avg       0.23      0.29      0.25       102\n",
      "weighted avg       0.46      0.56      0.50       102\n",
      "\n",
      "Accuracy: 0.5588235294117647\n",
      "Confusion Matrix:\n",
      "[[ 0  2  3  0  0]\n",
      " [ 0 25  7  0  0]\n",
      " [ 0 16 32  1  0]\n",
      " [ 0  3 11  0  0]\n",
      " [ 0  1  0  1  0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.00      0.00      0.00         5\n",
      "           5       0.53      0.78      0.63        32\n",
      "           6       0.60      0.65      0.63        49\n",
      "           7       0.00      0.00      0.00        14\n",
      "           8       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.56       102\n",
      "   macro avg       0.23      0.29      0.25       102\n",
      "weighted avg       0.46      0.56      0.50       102\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6078431372549019\n",
      "Confusion Matrix:\n",
      "[[ 0  1  0  0  0  0]\n",
      " [ 0  0  1  1  0  0]\n",
      " [ 0  0 41  4  1  0]\n",
      " [ 0  1 18 18  2  0]\n",
      " [ 0  0  1  8  3  0]\n",
      " [ 0  0  0  2  0  0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       0.67      0.89      0.77        46\n",
      "           6       0.55      0.46      0.50        39\n",
      "           7       0.50      0.25      0.33        12\n",
      "           8       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.61       102\n",
      "   macro avg       0.29      0.27      0.27       102\n",
      "weighted avg       0.57      0.61      0.58       102\n",
      "\n",
      "Accuracy: 0.6078431372549019\n",
      "Confusion Matrix:\n",
      "[[ 0  1  0  0  0  0]\n",
      " [ 0  0  1  1  0  0]\n",
      " [ 0  0 41  4  1  0]\n",
      " [ 0  1 18 18  2  0]\n",
      " [ 0  0  1  8  3  0]\n",
      " [ 0  0  0  2  0  0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       0.67      0.89      0.77        46\n",
      "           6       0.55      0.46      0.50        39\n",
      "           7       0.50      0.25      0.33        12\n",
      "           8       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.61       102\n",
      "   macro avg       0.29      0.27      0.27       102\n",
      "weighted avg       0.57      0.61      0.58       102\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6274509803921569\n",
      "Confusion Matrix:\n",
      "[[ 0  0  1  0  0  0]\n",
      " [ 0  0  2  1  0  0]\n",
      " [ 0  0 33  6  0  0]\n",
      " [ 0  0 14 29  0  0]\n",
      " [ 0  0  3  9  2  0]\n",
      " [ 0  0  0  1  1  0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.00      0.00      0.00         3\n",
      "           5       0.62      0.85      0.72        39\n",
      "           6       0.63      0.67      0.65        43\n",
      "           7       0.67      0.14      0.24        14\n",
      "           8       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.63       102\n",
      "   macro avg       0.32      0.28      0.27       102\n",
      "weighted avg       0.60      0.63      0.58       102\n",
      "\n",
      "Accuracy: 0.6274509803921569\n",
      "Confusion Matrix:\n",
      "[[ 0  0  1  0  0  0]\n",
      " [ 0  0  2  1  0  0]\n",
      " [ 0  0 33  6  0  0]\n",
      " [ 0  0 14 29  0  0]\n",
      " [ 0  0  3  9  2  0]\n",
      " [ 0  0  0  1  1  0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.00      0.00      0.00         3\n",
      "           5       0.62      0.85      0.72        39\n",
      "           6       0.63      0.67      0.65        43\n",
      "           7       0.67      0.14      0.24        14\n",
      "           8       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.63       102\n",
      "   macro avg       0.32      0.28      0.27       102\n",
      "weighted avg       0.60      0.63      0.58       102\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6470588235294118\n",
      "Confusion Matrix:\n",
      "[[ 0  0  2  0  0  0]\n",
      " [ 0  0  3  1  0  0]\n",
      " [ 0  0 30  7  1  0]\n",
      " [ 0  0 10 32  1  0]\n",
      " [ 0  0  3  6  4  0]\n",
      " [ 0  0  0  2  0  0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.00      0.00      0.00         4\n",
      "           5       0.62      0.79      0.70        38\n",
      "           6       0.67      0.74      0.70        43\n",
      "           7       0.67      0.31      0.42        13\n",
      "           8       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.65       102\n",
      "   macro avg       0.33      0.31      0.30       102\n",
      "weighted avg       0.60      0.65      0.61       102\n",
      "\n",
      "Accuracy: 0.6470588235294118\n",
      "Confusion Matrix:\n",
      "[[ 0  0  2  0  0  0]\n",
      " [ 0  0  3  1  0  0]\n",
      " [ 0  0 30  7  1  0]\n",
      " [ 0  0 10 32  1  0]\n",
      " [ 0  0  3  6  4  0]\n",
      " [ 0  0  0  2  0  0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.00      0.00      0.00         4\n",
      "           5       0.62      0.79      0.70        38\n",
      "           6       0.67      0.74      0.70        43\n",
      "           7       0.67      0.31      0.42        13\n",
      "           8       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.65       102\n",
      "   macro avg       0.33      0.31      0.30       102\n",
      "weighted avg       0.60      0.65      0.61       102\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6470588235294118\n",
      "Confusion Matrix:\n",
      "[[ 0  2  0  0]\n",
      " [ 0 33  7  1]\n",
      " [ 0 17 31  0]\n",
      " [ 0  2  7  2]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       0.61      0.80      0.69        41\n",
      "           6       0.69      0.65      0.67        48\n",
      "           7       0.67      0.18      0.29        11\n",
      "\n",
      "    accuracy                           0.65       102\n",
      "   macro avg       0.49      0.41      0.41       102\n",
      "weighted avg       0.64      0.65      0.62       102\n",
      "\n",
      "Accuracy: 0.6470588235294118\n",
      "Confusion Matrix:\n",
      "[[ 0  2  0  0]\n",
      " [ 0 33  7  1]\n",
      " [ 0 17 31  0]\n",
      " [ 0  2  7  2]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       0.61      0.80      0.69        41\n",
      "           6       0.69      0.65      0.67        48\n",
      "           7       0.67      0.18      0.29        11\n",
      "\n",
      "    accuracy                           0.65       102\n",
      "   macro avg       0.49      0.41      0.41       102\n",
      "weighted avg       0.64      0.65      0.62       102\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6568627450980392\n",
      "Confusion Matrix:\n",
      "[[ 0  4  2  0  0]\n",
      " [ 0 35 11  0  0]\n",
      " [ 1  9 29  1  0]\n",
      " [ 0  2  4  3  0]\n",
      " [ 0  0  1  0  0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.00      0.00      0.00         6\n",
      "           5       0.70      0.76      0.73        46\n",
      "           6       0.62      0.72      0.67        40\n",
      "           7       0.75      0.33      0.46         9\n",
      "           8       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.66       102\n",
      "   macro avg       0.41      0.36      0.37       102\n",
      "weighted avg       0.62      0.66      0.63       102\n",
      "\n",
      "Accuracy: 0.6568627450980392\n",
      "Confusion Matrix:\n",
      "[[ 0  4  2  0  0]\n",
      " [ 0 35 11  0  0]\n",
      " [ 1  9 29  1  0]\n",
      " [ 0  2  4  3  0]\n",
      " [ 0  0  1  0  0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.00      0.00      0.00         6\n",
      "           5       0.70      0.76      0.73        46\n",
      "           6       0.62      0.72      0.67        40\n",
      "           7       0.75      0.33      0.46         9\n",
      "           8       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.66       102\n",
      "   macro avg       0.41      0.36      0.37       102\n",
      "weighted avg       0.62      0.66      0.63       102\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5784313725490197\n",
      "Confusion Matrix:\n",
      "[[ 0  0  1  0  0  0]\n",
      " [ 0  0  3  1  0  0]\n",
      " [ 0  0 37 11  0  0]\n",
      " [ 0  0 13 15  2  2]\n",
      " [ 0  0  2  7  7  0]\n",
      " [ 0  0  0  0  1  0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.00      0.00      0.00         4\n",
      "           5       0.66      0.77      0.71        48\n",
      "           6       0.44      0.47      0.45        32\n",
      "           7       0.70      0.44      0.54        16\n",
      "           8       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.58       102\n",
      "   macro avg       0.30      0.28      0.28       102\n",
      "weighted avg       0.56      0.58      0.56       102\n",
      "\n",
      "Accuracy: 0.5784313725490197\n",
      "Confusion Matrix:\n",
      "[[ 0  0  1  0  0  0]\n",
      " [ 0  0  3  1  0  0]\n",
      " [ 0  0 37 11  0  0]\n",
      " [ 0  0 13 15  2  2]\n",
      " [ 0  0  2  7  7  0]\n",
      " [ 0  0  0  0  1  0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.00      0.00      0.00         4\n",
      "           5       0.66      0.77      0.71        48\n",
      "           6       0.44      0.47      0.45        32\n",
      "           7       0.70      0.44      0.54        16\n",
      "           8       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.58       102\n",
      "   macro avg       0.30      0.28      0.28       102\n",
      "weighted avg       0.56      0.58      0.56       102\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5882352941176471\n",
      "Confusion Matrix:\n",
      "[[ 0  1  1  0  0]\n",
      " [ 0 33 11  0  0]\n",
      " [ 0 18 24  0  0]\n",
      " [ 0  1  9  3  0]\n",
      " [ 0  0  1  0  0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       0.62      0.75      0.68        44\n",
      "           6       0.52      0.57      0.55        42\n",
      "           7       1.00      0.23      0.38        13\n",
      "           8       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.59       102\n",
      "   macro avg       0.43      0.31      0.32       102\n",
      "weighted avg       0.61      0.59      0.57       102\n",
      "\n",
      "Accuracy: 0.5882352941176471\n",
      "Confusion Matrix:\n",
      "[[ 0  1  1  0  0]\n",
      " [ 0 33 11  0  0]\n",
      " [ 0 18 24  0  0]\n",
      " [ 0  1  9  3  0]\n",
      " [ 0  0  1  0  0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       0.62      0.75      0.68        44\n",
      "           6       0.52      0.57      0.55        42\n",
      "           7       1.00      0.23      0.38        13\n",
      "           8       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.59       102\n",
      "   macro avg       0.43      0.31      0.32       102\n",
      "weighted avg       0.61      0.59      0.57       102\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6862745098039216\n",
      "Confusion Matrix:\n",
      "[[ 0  1  0  0  0]\n",
      " [ 0 39  5  0  0]\n",
      " [ 0 16 25  4  0]\n",
      " [ 0  3  2  6  0]\n",
      " [ 0  0  1  0  0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       0.66      0.89      0.76        44\n",
      "           6       0.76      0.56      0.64        45\n",
      "           7       0.60      0.55      0.57        11\n",
      "           8       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.69       102\n",
      "   macro avg       0.40      0.40      0.39       102\n",
      "weighted avg       0.68      0.69      0.67       102\n",
      "\n",
      "Accuracy: 0.6862745098039216\n",
      "Confusion Matrix:\n",
      "[[ 0  1  0  0  0]\n",
      " [ 0 39  5  0  0]\n",
      " [ 0 16 25  4  0]\n",
      " [ 0  3  2  6  0]\n",
      " [ 0  0  1  0  0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       0.66      0.89      0.76        44\n",
      "           6       0.76      0.56      0.64        45\n",
      "           7       0.60      0.55      0.57        11\n",
      "           8       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.69       102\n",
      "   macro avg       0.40      0.40      0.39       102\n",
      "weighted avg       0.68      0.69      0.67       102\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5882352941176471\n",
      "Confusion Matrix:\n",
      "[[ 0  0  1  2  0  0]\n",
      " [ 0  1  0  3  0  0]\n",
      " [ 0  0 36 12  1  0]\n",
      " [ 0  0  9 21  2  1]\n",
      " [ 0  0  0  9  2  0]\n",
      " [ 0  0  0  1  1  0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       1.00      0.25      0.40         4\n",
      "           5       0.78      0.73      0.76        49\n",
      "           6       0.44      0.64      0.52        33\n",
      "           7       0.33      0.18      0.24        11\n",
      "           8       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.59       102\n",
      "   macro avg       0.43      0.30      0.32       102\n",
      "weighted avg       0.59      0.59      0.57       102\n",
      "\n",
      "Accuracy: 0.5882352941176471\n",
      "Confusion Matrix:\n",
      "[[ 0  0  1  2  0  0]\n",
      " [ 0  1  0  3  0  0]\n",
      " [ 0  0 36 12  1  0]\n",
      " [ 0  0  9 21  2  1]\n",
      " [ 0  0  0  9  2  0]\n",
      " [ 0  0  0  1  1  0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       1.00      0.25      0.40         4\n",
      "           5       0.78      0.73      0.76        49\n",
      "           6       0.44      0.64      0.52        33\n",
      "           7       0.33      0.18      0.24        11\n",
      "           8       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.59       102\n",
      "   macro avg       0.43      0.30      0.32       102\n",
      "weighted avg       0.59      0.59      0.57       102\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "size = math.floor(data['class'].count() / 10)\n",
    "fold = []\n",
    "for i in range(10):\n",
    "    df = data.iloc[:size, :]\n",
    "    fold.append(df)\n",
    "    data = data.iloc[size:, :]\n",
    "    \n",
    "for i in range(len(fold)):\n",
    "    train = pd.DataFrame()\n",
    "    for j in range(len(fold)):\n",
    "        if j != i:\n",
    "            train = train.append(fold[j])\n",
    "    test = fold[i]\n",
    "    forest = Decision_Tree(train, test, train.columns)\n",
    "    acc_tmp = fit(test, forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5993485342019544\n",
      "Confusion Matrix:\n",
      "[[ 0  0  3  0  0  0]\n",
      " [ 0  0  7  3  0  0]\n",
      " [ 1  0 90 36  1  0]\n",
      " [ 0  0 34 82  8  1]\n",
      " [ 0  0  5 20 12  0]\n",
      " [ 0  0  0  2  2  0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.00      0.00      0.00        10\n",
      "           5       0.65      0.70      0.67       128\n",
      "           6       0.57      0.66      0.61       125\n",
      "           7       0.52      0.32      0.40        37\n",
      "           8       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.60       307\n",
      "   macro avg       0.29      0.28      0.28       307\n",
      "weighted avg       0.57      0.60      0.58       307\n",
      "\n",
      "Accuracy: 0.5993485342019544\n",
      "Confusion Matrix:\n",
      "[[ 0  0  3  0  0  0]\n",
      " [ 0  0  7  3  0  0]\n",
      " [ 1  0 90 36  1  0]\n",
      " [ 0  0 34 82  8  1]\n",
      " [ 0  0  5 20 12  0]\n",
      " [ 0  0  0  2  2  0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.00      0.00      0.00        10\n",
      "           5       0.65      0.70      0.67       128\n",
      "           6       0.57      0.66      0.61       125\n",
      "           7       0.52      0.32      0.40        37\n",
      "           8       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.60       307\n",
      "   macro avg       0.29      0.28      0.28       307\n",
      "weighted avg       0.57      0.60      0.58       307\n",
      "\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "df_x = pd.read_excel(\"Dataset1/train/X_train.xlsx\")\n",
    "df_y = pd.read_excel(\"Dataset1/train/y_train.xlsx\")\n",
    "#df_x = fill_missing_value_mean(df_x)\n",
    "df_x = normal(df_x)\n",
    "df_x = distance(df_x)\n",
    "test_set = pd.read_excel(\"Dataset1_test/X_test.xlsx\")\n",
    "test_set = fill_missing_value_mean(test_set)\n",
    "\n",
    "ss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.3, train_size = 0.7, random_state = 0)\n",
    "train = pd.DataFrame()\n",
    "test  = pd.DataFrame()\n",
    "for train_idx, test_idx in ss.split(df_x, df_y):\n",
    "    for j in train_idx:\n",
    "        train = train.append(df_x[j:j + 1].join(df_y[j:j + 1]))\n",
    "    for j in test_idx:\n",
    "        test = test.append(df_x[j:j + 1].join(df_y[j:j + 1]))\n",
    "\n",
    "model = Decision_Tree(train, test, train.columns)\n",
    "acc = fit(test, model)\n",
    "\n",
    "predict_ans = pd.DataFrame()\n",
    "for i, row in test_set.iterrows():\n",
    "    predict = []\n",
    "    for j in model:\n",
    "        re = apply(row, j)\n",
    "        predict.append(re)\n",
    "    #print(predict)\n",
    "    vote = max(set(predict), key = predict.count)\n",
    "    tmp = pd.DataFrame([vote], columns = ['return'])\n",
    "    predict_ans = predict_ans.append(tmp)\n",
    "    \n",
    "writer = pd.ExcelWriter('./test_predict_dataset1.xlsx')\n",
    "predict_ans.to_excel(writer,index=False)\n",
    "print('done')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='bs4')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    reviews = []\n",
    "    for raw in tqdm(df['Phrase']):\n",
    "        text = re.sub('[^a-zA-Z]', ' ', str(raw))\n",
    "        words = word_tokenize(text.lower())\n",
    "        stops = set(stopwords.words('english'))\n",
    "        non_stopwords = [word for word in words if not word in stops]\n",
    "        lemma_words = [lemmatizer.lemmatize(word) for word in non_stopwords]    \n",
    "        reviews.append(lemma_words)\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_preprocess(list_X_train, list_X_val, list_test):\n",
    "    unique_words = set()\n",
    "    len_max = 0\n",
    "    for sent in tqdm(list_X_train):\n",
    "        unique_words.update(sent)\n",
    "        if len_max < len(sent):\n",
    "            len_max = len(sent)\n",
    "    len(list(unique_words)), len_max\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=len(list(unique_words)))\n",
    "    tokenizer.fit_on_texts(list(list_X_train))\n",
    "     \n",
    "    X_train = tokenizer.texts_to_sequences(list_X_train)\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=len_max)\n",
    "\n",
    "    X_val = tokenizer.texts_to_sequences(list_X_val)\n",
    "    X_val = sequence.pad_sequences(X_val, maxlen=len_max)\n",
    "    \n",
    "    X_test = tokenizer.texts_to_sequences(list_test)\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=len_max)\n",
    "\n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = pd.read_excel(\"Dataset2/Dataset2_train/X_train.xlsx\")\n",
    "df_y = pd.read_excel(\"Dataset2/Dataset2_train/y_train.xlsx\")\n",
    "test_set = pd.read_excel(\"Dataset2_test/X_test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 124848/124848 [01:09<00:00, 1800.19it/s]\n",
      "100%|| 31212/31212 [00:16<00:00, 1910.74it/s]\n"
     ]
    }
   ],
   "source": [
    "df_x = preprocess_data(df_x)\n",
    "X_test = preprocess_data(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df_x, df_y, test_size=0.2, stratify=df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 99878/99878 [00:00<00:00, 600226.80it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_, X_val_, X_test_ = tokenizer_preprocess(X_train, X_val, X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_list = list(y_train['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 99878/99878 [07:22<00:00, 225.90it/s]\n"
     ]
    }
   ],
   "source": [
    "feature_set = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9','f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'class']\n",
    "train = pd.DataFrame()\n",
    "test  = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(len(X_train_))):\n",
    "    feature_val = []\n",
    "    for j in range(29):\n",
    "        feature_val.append(X_train_[i][j])\n",
    "    feature_val.append(y_list[i])\n",
    "    tmp = pd.DataFrame([feature_val], columns = feature_set)\n",
    "    train = train.append(tmp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 31212/31212 [01:14<00:00, 421.08it/s]\n"
     ]
    }
   ],
   "source": [
    "feature_test = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9','f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28']\n",
    "y_set = pd.DataFrame()\n",
    "for i in tqdm(range(len(X_test_))):\n",
    "    feature_val = []\n",
    "    for j in range(29):\n",
    "        feature_val.append(X_test_[i][j])\n",
    "    tmp = pd.DataFrame([feature_val], columns = feature_test)\n",
    "    y_set = y_set.append(tmp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       f0  f1  f2  f3  f4  f5  f6  f7  f8  f9  ...  f19  f20  f21   f22   f23  \\\n",
      "0       0   0   0   0   0   0   0   0   0   0  ...   93   16    9  1081  2015   \n",
      "1       0   0   0   0   0   0   0   0   0   0  ...    0    0    0     0     0   \n",
      "2       0   0   0   0   0   0   0   0   0   0  ...    0    0    0     0     0   \n",
      "3       0   0   0   0   0   0   0   0   0   0  ...    0    0    0     0     0   \n",
      "4       0   0   0   0   0   0   0   0   0   0  ...    0    0    0     0     0   \n",
      "...    ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ...  ...  ...   ...   ...   \n",
      "31207   0   0   0   0   0   0   0   0   0   0  ...    0    0    0     0     0   \n",
      "31208   0   0   0   0   0   0   0   0   0   0  ...    0    0    0     0     0   \n",
      "31209   0   0   0   0   0   0   0   0   0   0  ...    0    0    0     0     0   \n",
      "31210   0   0   0   0   0   0   0   0   0   0  ...    0    0    0     0     0   \n",
      "31211   0   0   0   0   0   0   0   0   0   0  ...    0    0    0     0     0   \n",
      "\n",
      "        f24    f25   f26   f27    f28  \n",
      "0      5999   2032   240  1970   1189  \n",
      "1         0      0  1006   100     75  \n",
      "2         0    910     1  2014    675  \n",
      "3       797   4341    69   459     78  \n",
      "4         0      0  1912   729   2445  \n",
      "...     ...    ...   ...   ...    ...  \n",
      "31207     0      0     0   152   3962  \n",
      "31208     0      0     0     0  12231  \n",
      "31209     0      0     0     0    152  \n",
      "31210  3326  12107   105   106   8053  \n",
      "31211     0      0     0  1475   1059  \n",
      "\n",
      "[31212 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "print(y_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 24970/24970 [01:01<00:00, 409.33it/s]\n"
     ]
    }
   ],
   "source": [
    "y_list = list(y_val['Sentiment'])\n",
    "\n",
    "for i in tqdm(range(len(X_val_))):\n",
    "    feature_val = []\n",
    "    for j in range(29):\n",
    "        feature_val.append(X_val_[i][j])\n",
    "    feature_val.append(y_list[i])\n",
    "    tmp = pd.DataFrame([feature_val], columns = feature_set)\n",
    "    test = test.append(tmp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5559070885062074\n",
      "Confusion Matrix:\n",
      "[[  144   329   564    87     8]\n",
      " [  171  1008  2927   239    19]\n",
      " [   69   627 11518   497    22]\n",
      " [   35   317  3736  1070   110]\n",
      " [   19   101   831   381   141]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.13      0.18      1132\n",
      "           1       0.42      0.23      0.30      4364\n",
      "           2       0.59      0.90      0.71     12733\n",
      "           3       0.47      0.20      0.28      5268\n",
      "           4       0.47      0.10      0.16      1473\n",
      "\n",
      "    accuracy                           0.56     24970\n",
      "   macro avg       0.46      0.31      0.33     24970\n",
      "weighted avg       0.52      0.56      0.49     24970\n",
      "\n",
      "Accuracy: 0.5559070885062074\n",
      "Confusion Matrix:\n",
      "[[  144   329   564    87     8]\n",
      " [  171  1008  2927   239    19]\n",
      " [   69   627 11518   497    22]\n",
      " [   35   317  3736  1070   110]\n",
      " [   19   101   831   381   141]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.13      0.18      1132\n",
      "           1       0.42      0.23      0.30      4364\n",
      "           2       0.59      0.90      0.71     12733\n",
      "           3       0.47      0.20      0.28      5268\n",
      "           4       0.47      0.10      0.16      1473\n",
      "\n",
      "    accuracy                           0.56     24970\n",
      "   macro avg       0.46      0.31      0.33     24970\n",
      "weighted avg       0.52      0.56      0.49     24970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Decision_Tree(train, test, train.columns)\n",
    "acc = fit(test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31212it [00:23, 1307.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "predict_ans2 = pd.DataFrame()\n",
    "for i, row in tqdm(y_set.iterrows()):\n",
    "    predict2 = []\n",
    "    for j in model:\n",
    "        re = apply(row, j)\n",
    "        predict2.append(re)\n",
    "    vote = max(set(predict2), key = predict2.count)\n",
    "    tmp = pd.DataFrame([vote], columns = ['return'])\n",
    "    predict_ans2 = predict_ans2.append(tmp)\n",
    "    \n",
    "writer = pd.ExcelWriter('./test_predict_dataset2.xlsx')\n",
    "predict_ans2.to_excel(writer,index=False)\n",
    "print('done')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "df_x = pd.read_excel(\"Dataset1/train/X_train.xlsx\")\n",
    "df_y = pd.read_excel(\"Dataset1/train/y_train.xlsx\")\n",
    "#df_x = fill_missing_value_mean(df_x)\n",
    "df_x = normal(df_x)\n",
    "df_x = distance(df_x)\n",
    "test_set = pd.read_excel(\"Dataset1_test/X_test.xlsx\")\n",
    "test_set = fill_missing_value_mean(test_set)\n",
    "\n",
    "ss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.3, train_size = 0.7, random_state = 0)\n",
    "train = pd.DataFrame()\n",
    "test  = pd.DataFrame()\n",
    "for train_idx, test_idx in ss.split(df_x, df_y):\n",
    "    for j in train_idx:\n",
    "        train = train.append(df_x[j:j + 1].join(df_y[j:j + 1]))\n",
    "    for j in test_idx:\n",
    "        test = test.append(df_x[j:j + 1].join(df_y[j:j + 1]))\n",
    "\n",
    "writer = pd.ExcelWriter('./train_relation_dataset1.xlsx')\n",
    "train.corr().to_excel(writer,index=False)\n",
    "print('done')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
